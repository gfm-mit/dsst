[find_lr]
  min_lr=3e-2
  max_lr=1e2
  max_epochs=200
 
[tune]
  #learning_rate={low=8e-2, high=1.5e-1, steps=5}
  #momentum=[0, 0.5, 0.9, 0.999]
  agg_kind=["meanmax", "max", "mean", "last"]

[compare.prodigy]
  # gives out around step 5
  scheduler='none'
  optimizer='prodigy'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_epochs=5
  max_epochs=15
  learning_rate=1

[compare.samadam]
  # basically excellent
  scheduler='none'
  optimizer='samadam'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_epochs=5
  max_epochs=15
  learning_rate=1e-1

[compare.samsgd]
  # would be surprised if this were better anywhere nonconvex
  scheduler='none'
  optimizer='samsgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_epochs=5
  max_epochs=15
  learning_rate=1.3e-1

[compare.sgd]
  # check just for sanity
  scheduler='none'
  optimizer='sgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_epochs=5
  max_epochs=15
  learning_rate=1.3e-1