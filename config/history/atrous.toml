[sgd]
  # competitive, oddly enough
  scheduler='none'
  optimizer='sgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[onecyclesgd]
  # better than sgd, but annoying
  scheduler='onecycle'
  optimizer='sgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[sfsgd]
  # a little slower than hand-tuned one-cycle sgd, but nice
  scheduler='none'
  optimizer='sfsgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[samsgd]
  # scary good.  crosses .85 on validation after 800 steps
  scheduler='none'
  optimizer='samsgd'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=2e-1

[adam]
  # not helpful for convex
  scheduler='none'
  optimizer='adam'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[onecycleadam]
  scheduler='onecycle'
  optimizer='adam'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[sfadam]
  # adam not so great on convex
  scheduler='none'
  optimizer='sfadam'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=3e-1

[samadam]
  # useless on convex problem, apparently
  scheduler='none'
  optimizer='samadam'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=2e-1

[lion]
  # terrible performance
  scheduler='none'
  optimizer='lion'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=2e-1

[prodigy]
  # nothing special, but nice not to have to tune learning rate
  scheduler='none'
  optimizer='prodigy'
  weight_decay=0
  momentum=0.9
  conditioning_smoother=0.999
  warmup_steps=5
  max_epochs=80
  min_epochs=3
  learning_rate=1